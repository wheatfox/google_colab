{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tmp",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWz8E9bWyZfMZG5/sK3w1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wheatfox/google_colab/blob/main/tmp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79r32zu2u0Eg",
        "outputId": "bdcd8aaa-7794-48b2-c97e-822444d8af2e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.meta     tmp\n",
            "bert_model.ckpt.data-00000-of-00001  chinese_L-12_H-768_A-12  tmp.py\n",
            "bert_model.ckpt.index\t\t     sample_data\t      vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG6Fmcw6vM1n"
      },
      "source": [
        "# test github updating\n",
        "print(\"hello github\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACfagdvnJp2L",
        "outputId": "8a40b6d6-64ce-4246-90da-233a9eb040cd"
      },
      "source": [
        "def test_ner_on_raw_keras():\n",
        "    import tensorflow as tf\n",
        "    # import tensorflow.compat.v1 as v1\n",
        "    import tensorflow_addons as tfa\n",
        "    import numpy as np\n",
        "    from pathlib import Path\n",
        "    import math\n",
        "    import bert as Bert\n",
        "    from datetime import datetime\n",
        "    import random\n",
        "    random.seed(0)\n",
        "    #\n",
        "    _UNKNOWN_ = \"un_known\"\n",
        "\n",
        "    def get_data(f):\n",
        "        with Path(f).open(\"r\", encoding=\"utf8\")as fi:\n",
        "            vocab, tags = [], []\n",
        "            for line in fi.readlines():\n",
        "                parts = line.strip().split(\" \")\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "                vocab.append(parts[0])\n",
        "                tags.append(parts[1])\n",
        "            vocab = list(set(vocab))\n",
        "            tags = list(set(tags))\n",
        "        #\n",
        "        vocab2id = dict([(w, i + 1) for i, w in enumerate(vocab)])\n",
        "        vocab2id.update({_UNKNOWN_: 0})\n",
        "        tag2id = dict([(t, j + 1) for j, t in enumerate(tags)])\n",
        "        tag2id.update({_UNKNOWN_: 0})\n",
        "        return vocab2id, tag2id\n",
        "\n",
        "    def crf_generator(f, batch_size, max_length, vocab2id, tag2id):\n",
        "        data, tags, lengths = [], [], []\n",
        "        count = 0\n",
        "        with Path(f).open('r', encoding=\"utf8\") as fi:\n",
        "            curr_x, curr_y = [], []\n",
        "            for line in fi.readlines():\n",
        "                if \"\\n\" == line:\n",
        "                    if (count + 1) % batch_size == 0:\n",
        "                        yield [np.array(data), np.array(lengths), np.array(tags)], np.array(tags)\n",
        "                        data, lengths, tags = [], [], []\n",
        "                    data.append([curr_x[i] if i < len(curr_x) else 0\n",
        "                                 for i in range(max_length)])\n",
        "                    tags.append([curr_y[i] if i < len(curr_y) else tag2id.get(\"O\")\n",
        "                                 for i in range(max_length)])\n",
        "                    lengths.append(len(curr_x))\n",
        "                    count += 1\n",
        "                    curr_x, curr_y = [], []\n",
        "                else:\n",
        "                    parts = line.strip().split(\" \")\n",
        "                    if len(parts) != 2:\n",
        "                        print(\"数据错误：{}\".format(line), file=sys.stderr)\n",
        "                    else:\n",
        "                        curr_x.append(vocab2id.get(parts[0], 0))\n",
        "                        curr_y.append(tag2id.get(parts[1]))\n",
        "\n",
        "    class CrfGenerator(tf.keras.utils.Sequence):\n",
        "        def __init__(self, f, batch_size, max_length, vocab2id, tag2id):\n",
        "            # super(CrfGenerator, self).__init__()\n",
        "            # inputs = self.generator_fn_from_file(f)\n",
        "            data, tags, lengths = [], [], []\n",
        "            with Path(f).open('r', encoding=\"utf8\") as fi:\n",
        "                curr_x, curr_y = [], []\n",
        "                for line in fi.readlines():\n",
        "                    if \"\\n\" == line:\n",
        "                        data.append([curr_x[i] if i < len(curr_x) else 0\n",
        "                                     for i in range(max_length)])\n",
        "                        tags.append([curr_y[i] if i < len(curr_y) else 0\n",
        "                                     for i in range(max_length)])\n",
        "                        lengths.append(len(curr_x))\n",
        "                        curr_x, curr_y = [], []\n",
        "                    else:\n",
        "                        parts = line.strip().split(\" \")\n",
        "                        if len(parts) != 2:\n",
        "                            print(\"数据错误：{}\".format(line), file=sys.stderr)\n",
        "                        else:\n",
        "                            curr_x.append(vocab2id.get(parts[0], 0))\n",
        "                            curr_y.append(tag2id.get(parts[1], 0))\n",
        "            #\n",
        "            self.inputs = [data, lengths, tags]\n",
        "            self.batch_size = len(self.inputs[0]) if batch_size is None else batch_size\n",
        "\n",
        "        def __len__(self):\n",
        "            return math.ceil(len(self.inputs[0]) / self.batch_size)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            curr_data = np.array(self.inputs[0][idx * self.batch_size: (idx + 1) * self.batch_size])\n",
        "            curr_lengths = np.array(self.inputs[1][idx * self.batch_size: (idx + 1) * self.batch_size])\n",
        "            curr_tags = np.array(self.inputs[2][idx * self.batch_size: (idx + 1) * self.batch_size])\n",
        "            # curr_lengths = np.reshape(curr_lengths, (-1, 1))\n",
        "            return [curr_data, curr_lengths, curr_tags], curr_tags\n",
        "\n",
        "        def shuffle(self):\n",
        "            length = len(self.inputs[0])\n",
        "            indexes = [i for i in range(length)]\n",
        "            random.shuffle(indexes)\n",
        "            self.inputs = [[items[j] for j in indexes] for items in self.inputs]\n",
        "\n",
        "    class CrfLoss(tf.keras.layers.Layer):\n",
        "        def __init__(self, **kwargs):\n",
        "            super(CrfLoss, self).__init__(**kwargs)\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {}\n",
        "            base_config = super(CrfLoss, self).get_config()\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        def call(self, inputs=tf.keras.Input(shape=(None, None)), **kwargs):\n",
        "            tags, lengths, params = kwargs.get(\"tags\", tf.keras.Input(shape=(None,))), \\\n",
        "                                    kwargs.get(\"lengths\", tf.keras.Input(shape=())), \\\n",
        "                                    kwargs.get(\"params\", tf.keras.Input(shape=(None,)))\n",
        "            # log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(data,\n",
        "            #                                                       tags, lengths, params)\n",
        "            log_likelihood, _ = tfa.text.crf_log_likelihood(inputs,\n",
        "                                                            tags, lengths, params)\n",
        "            loss = tf.reduce_sum(-log_likelihood)\n",
        "            # loss = tf.keras.losses.sparse_categorical_crossentropy(tags, outputs, from_logits=True)\n",
        "            self.add_loss(loss)\n",
        "            return inputs\n",
        "\n",
        "    class CrfMetrics(tf.keras.layers.Layer):\n",
        "        def __init__(self, **kwargs):\n",
        "            super(CrfMetrics, self).__init__(**kwargs)\n",
        "            self.accuracy_func = tf.keras.metrics.Accuracy()\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {}\n",
        "            base_config = super(CrfMetrics, self).get_config()\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        def call(self, inputs=tf.keras.Input(shape=(None, None)), **kwargs):\n",
        "            tags, lengths, params = kwargs.get(\"tags\", tf.keras.Input(shape=(None,))), \\\n",
        "                                    kwargs.get(\"lengths\", tf.keras.Input(shape=())), \\\n",
        "                                    kwargs.get(\"params\", tf.keras.Input(shape=(None,)))\n",
        "            lengths = tf.cast(lengths, dtype=tf.int32)\n",
        "            pred_ids, _ = tfa.text.crf_decode(inputs,\n",
        "                                              params,\n",
        "                                              lengths)\n",
        "            tags = tf.cast(tags, dtype=tf.int32)\n",
        "            pred_ids = tf.cast(pred_ids, dtype=tf.int32)\n",
        "            # print(tags, pred_ids, lengths)\n",
        "            result = self.accuracy_func(tags, pred_ids)\n",
        "            self.add_metric(result, name=\"accuracy\")\n",
        "            return inputs\n",
        "\n",
        "    class CrfLayer(tf.keras.layers.Layer):\n",
        "        def __init__(self, out_size=None, **kwargs):\n",
        "            super(CrfLayer, self).__init__(**kwargs)\n",
        "            v_init = tf.random_normal_initializer()\n",
        "            self.crf_params = tf.Variable(initial_value=v_init(shape=(out_size, out_size), dtype=tf.float32),\n",
        "                                          name=\"crf_params\",\n",
        "                                          trainable=True)\n",
        "            self.accuracy_func = tf.keras.metrics.Accuracy()\n",
        "            self.out_size = out_size\n",
        "\n",
        "        def get_trans_params(self):\n",
        "            return self.crf_params\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {\"out_size\": self.out_size}\n",
        "            base_config = super(CrfLayer, self).get_config()\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        def call(self, inputs, lengths, tags):\n",
        "            # tags, lengths = kwargs.get(\"tags\", None), kwargs.get(\"lengths\", None)\n",
        "            self.add_loss(self.compute_loss(inputs, lengths, tags))\n",
        "            #\n",
        "            acc = self.compute_accuracy(inputs, lengths, tags)\n",
        "            self.add_metric(acc, name=\"accuracy\")\n",
        "            return inputs\n",
        "\n",
        "        def compute_accuracy(self, inputs, lengths, tags):\n",
        "            lengths = tf.cast(lengths, dtype=tf.int32)\n",
        "            pred_ids = self.decode(inputs, lengths)\n",
        "            tags = tf.cast(tags, dtype=tf.int32)\n",
        "            pred_ids = tf.cast(pred_ids, dtype=tf.int32)\n",
        "            acc = self.accuracy_func(tags, pred_ids)\n",
        "            return acc\n",
        "\n",
        "        def compute_loss(self, inputs, lengths, tags):\n",
        "            log_likelihood, _ = tfa.text.crf_log_likelihood(inputs,\n",
        "                                                            tags, lengths, self.crf_params)\n",
        "            loss = tf.reduce_sum(-log_likelihood)\n",
        "            return loss\n",
        "\n",
        "        def decode(self, inputs, lengths):\n",
        "            pred_ids, _ = tfa.text.crf_decode(inputs,\n",
        "                                              self.crf_params,\n",
        "                                              lengths)\n",
        "            return pred_ids\n",
        "\n",
        "    class MyCallback(tf.keras.callbacks.Callback):\n",
        "        def __init__(self, model, valid_data, test_data, patience=20):\n",
        "            super(MyCallback, self).__init__()\n",
        "            self.top_model = model\n",
        "            self.patience = patience\n",
        "            self.valid = valid_data\n",
        "            self.test = test_data\n",
        "            #\n",
        "            self.best = np.Inf\n",
        "            self.best_weights = None\n",
        "            self.stopped_epoch = 0\n",
        "            self.waited = 0\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            # text = \"测试啊测试\"\n",
        "            # pred_tags = self.top_model.predict(text)\n",
        "            # # print(\"\\n\" + \", \".join([\"{}: {}\".format(k, v) for k, v in logs.items()]))\n",
        "            # print(\"\\nepoch：{}\\n文本：{}\\n输出：{}\".format(epoch+1, text, pred_tags))\n",
        "            #\n",
        "            logs = dict() if not isinstance(logs, dict) else logs\n",
        "            curr_loss = logs.get(\"loss\", 0)\n",
        "            if np.less(curr_loss, self.best):\n",
        "                self.waited = 0\n",
        "                self.best = curr_loss\n",
        "                self.top_model.save_model()\n",
        "                print(\"\\nFound better model at epoch-{}.\\n\".format(epoch))\n",
        "                self.best_weights = self.model.get_weights()\n",
        "                print(\"\\nStarting validation...\")\n",
        "                self.model.evaluate(self.valid[0], self.valid[1])\n",
        "                print(\"\\nFinished validation.\")\n",
        "                print(\"\\nStarting test...\")\n",
        "                self.model.evaluate(self.test[0], self.test[1])\n",
        "                print(\"\\nFinished test.\")\n",
        "            else:\n",
        "                self.waited += 1\n",
        "                if self.waited >= self.patience:\n",
        "                    self.stopped_epoch = epoch\n",
        "                    self.model.stop_training = True\n",
        "                    print(\"\\nRestore weights from best model.\\n\")\n",
        "                    self.model.set_weights(self.best_weights)\n",
        "                    self.top_model.save_model()\n",
        "\n",
        "        def on_train_end(self, logs=None):\n",
        "            if self.stopped_epoch > 0:\n",
        "                print(\"\\nEpoch %05d: early stopping.\\n\" % (self.stopped_epoch + 1))\n",
        "\n",
        "    class BlstmCrf(tf.keras.Model):\n",
        "        def __init__(self, params=None, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            params = dict() if params is None else params\n",
        "            self.layer_name_prefix = \"BlstmCrfModel_\"\n",
        "            max_seq_length = params.get(\"max_seq_len\", 128)\n",
        "            self.max_seq_len = max_seq_length\n",
        "            vocab2id = params.get(\"vocab2id\", None)\n",
        "            if vocab2id is None:\n",
        "                raise ValueError(\"vocab2id缺失!\")\n",
        "            vocab_size = len(vocab2id)\n",
        "            embed_size = params.get(\"embed_size\", 100)\n",
        "            hidden_size = params.get(\"hidden_size\", 100)\n",
        "            tag2id = params.get(\"tag2id\", None)\n",
        "            if tag2id is None:\n",
        "                raise ValueError(\"tg2id缺失!\")\n",
        "            out_size = len(tag2id)\n",
        "            self.vocab2id = vocab2id\n",
        "            self.tag2id = tag2id\n",
        "            self.id2tag = dict([(i, t) for t, i in self.tag2id.items()])\n",
        "            # pre_trained_model_path = params.get(\"pre_trained_model_path\", None)\n",
        "            # bert_params = Bert.params_from_pretrained_ckpt(pre_trained_model_path)\n",
        "            # self.bert = Bert.BertModelLayer.from_params(bert_params, name=self.layer_name_prefix + \"bert\")\n",
        "            #\n",
        "            self.embed = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                   output_dim=embed_size,\n",
        "                                                   input_shape=(None, max_seq_length),\n",
        "                                                   name=self.layer_name_prefix + \"embedding\",\n",
        "                                                   dtype=tf.float32,\n",
        "                                                   trainable=True)\n",
        "            self.blstm = tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.LSTM(units=hidden_size, return_sequences=True),\n",
        "                name=self.layer_name_prefix + \"blstm\",\n",
        "                dtype=tf.float32,\n",
        "                trainable=True\n",
        "            )\n",
        "            self.dense1 = tf.keras.layers.Dense(units=out_size, name=self.layer_name_prefix + \"projection\",\n",
        "                                                dtype=tf.float32, trainable=True)\n",
        "            self.crf_layer = CrfLayer(out_size=out_size, name=self.layer_name_prefix + \"crf\")\n",
        "\n",
        "        def call(self, inputs, training=None):\n",
        "            x, lengths, tags = inputs\n",
        "            x = self.embed(x, training=training)\n",
        "            # x = self.bert(x, training=training)\n",
        "            x = self.blstm(x, training=training)\n",
        "            x = self.dense1(x, training=training)\n",
        "            outputs = self.crf_layer(x, lengths, tags)\n",
        "            return outputs\n",
        "\n",
        "        def train_step(self, data):\n",
        "            # Unpack the data. Its structure depends on your model and\n",
        "            # on what you pass to `fit()`.\n",
        "            x, y = data\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                inputs, lengths, tags = x\n",
        "                logits = self(x, training=True)  # Forward pass\n",
        "                # Compute the loss value\n",
        "                # (the loss function is configured in `compile()`)\n",
        "                loss = self.crf_layer.compute_loss(logits, lengths, tags)\n",
        "\n",
        "            # Compute gradients\n",
        "            trainable_vars = self.trainable_variables\n",
        "            gradients = tape.gradient(loss, trainable_vars)\n",
        "            # Update weights\n",
        "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "            # Update metrics (includes the metric that tracks the loss)\n",
        "            self.compiled_metrics.update_state(y, y_pred)\n",
        "            # Return a dict mapping metric names to current value\n",
        "            return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "        def decode(self, inputs, lengths):\n",
        "            pred_ids = self.crf_layer.decode(inputs, lengths)\n",
        "            return pred_ids\n",
        "\n",
        "    class Ner:\n",
        "        def __init__(self, params=None):\n",
        "            self.layer_name_prefix = \"BlstmCrfModel_\"\n",
        "            params = params if params is not None else dict()\n",
        "            self.is_save_by_weights = params.get(\"is_save_by_weights\", True)\n",
        "            self.model_path = params.get(\"model_path\", \"model.h5\")\n",
        "            max_seq_length = params.get(\"max_seq_len\", 128)\n",
        "            self.max_seq_len = max_seq_length\n",
        "            vocab2id = params.get(\"vocab2id\", None)\n",
        "            if vocab2id is None:\n",
        "                raise ValueError(\"vocab2id缺失!\")\n",
        "            vocab_size = len(vocab2id)\n",
        "            embed_size = params.get(\"embed_size\", 100)\n",
        "            hidden_size = params.get(\"hidden_size\", 100)\n",
        "            tag2id = params.get(\"tag2id\", None)\n",
        "            if tag2id is None:\n",
        "                raise ValueError(\"tg2id缺失!\")\n",
        "            out_size = len(tag2id)\n",
        "            self.vocab2id = vocab2id\n",
        "            self.tag2id = tag2id\n",
        "            self.id2tag = dict([(i, t) for t, i in self.tag2id.items()])\n",
        "            #\n",
        "            pre_trained_model_path = params.get(\"pre_trained_model_path\", None)\n",
        "            bert_params = Bert.params_from_pretrained_ckpt(pre_trained_model_path)\n",
        "            self.embed = Bert.BertModelLayer.from_params(bert_params, name=self.layer_name_prefix + \"bert\")\n",
        "            # self.embed = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "            #                                        output_dim=embed_size,\n",
        "            #                                        input_shape=(None, max_seq_length),\n",
        "            #                                        name=self.layer_name_prefix + \"embedding\",\n",
        "            #                                        dtype=tf.float32,\n",
        "            #                                        trainable=True)\n",
        "            self.blstm = tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.LSTM(units=hidden_size, return_sequences=True),\n",
        "                name=self.layer_name_prefix + \"blstm\",\n",
        "                dtype=tf.float32,\n",
        "                trainable=True\n",
        "            )\n",
        "            self.dense1 = tf.keras.layers.Dense(units=out_size, name=self.layer_name_prefix + \"projection\",\n",
        "                                                dtype=tf.float32, trainable=True)\n",
        "            self.crf_layer = CrfLayer(out_size=out_size, name=self.layer_name_prefix + \"crf\")\n",
        "            # v_init = tf.random_normal_initializer()\n",
        "            # self.crf_params = tf.Variable(initial_value=v_init(shape=(out_size, out_size), dtype=tf.float32),\n",
        "            #                               name=self.layer_name_prefix + \"crf\",\n",
        "            #                               trainable=True)\n",
        "            # self.softmax = tf.keras.layers.Softmax()\n",
        "            # 创建模型\n",
        "            data, lengths = tf.keras.Input(shape=(max_seq_length,), name=self.layer_name_prefix + \"data\"), \\\n",
        "                            tf.keras.Input(shape=(), name=self.layer_name_prefix + \"lengths\")\n",
        "            tags = tf.keras.Input(shape=(max_seq_length,), name=self.layer_name_prefix + \"tags\")\n",
        "            # outputs = tf.keras.Input(shape=(max_seq_length, out_size), name=self.layer_name_prefix + \"outputs\")\n",
        "            # self.model = BlstmCrf(params)\n",
        "            embedded = self.embed(data)\n",
        "            blstm_outputs = self.blstm(embedded)\n",
        "            projection_outputs = self.dense1(blstm_outputs)\n",
        "            # outputs = self.softmax(projection_outputs)\n",
        "            outputs = self.crf_layer(projection_outputs, lengths, tags)\n",
        "            self.model = tf.keras.Model(inputs=[data, lengths, tags], outputs=outputs)\n",
        "\n",
        "            # self.model.add_loss(tfa.text.crf_log_likelihood(projection_outputs, tags, lengths, self.crf_params)[0])\n",
        "            #\n",
        "            # self.model.compile(optimizer=\"Adam\",\n",
        "            #                    loss=lambda y_true, y_pred: tf.reduce_sum(\n",
        "            #                        -tf.contrib.crf.crf_log_likelihood(\n",
        "            #                            y_pred, tf.cast(y_true, dtype=tf.int32), lengths, self.crf_params)[0]),)\n",
        "            lr_scheduler = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.001,\n",
        "                                                                          decay_steps=10, decay_rate=0.75)\n",
        "            self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler))\n",
        "            # self.model.compile(optimizer=\"Adam\", loss=crf_loss([lengths, self.crf_params]))\n",
        "            self.model.summary()\n",
        "\n",
        "        def fit(self, train_data_file, valid_data_file, test_data_file, batch_size=32, epochs=1, steps_per_epoch=None):\n",
        "            train_generator = CrfGenerator(train_data_file, batch_size, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "            valid_generator = CrfGenerator(valid_data_file, None, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "            valid_data = valid_generator.__getitem__(0)\n",
        "            test_generator = CrfGenerator(test_data_file, None, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "            test_data = test_generator.__getitem__(0)\n",
        "            print(\"获取数据, 训练/验证/测试, {}/{}/{}\".format(\n",
        "                len(train_generator)*batch_size, len(valid_data[-1]), len(test_data[-1])))\n",
        "            #\n",
        "            self.model.fit(train_generator,\n",
        "                           epochs=epochs,\n",
        "                           steps_per_epoch=len(data_generator) if steps_per_epoch is None else steps_per_epoch,\n",
        "                           callbacks=[MyCallback(self, valid_data=valid_data, test_data=test_data, patience=2)])\n",
        "            self.save_model()\n",
        "\n",
        "        def train(self, train_file, valid_file, test_file, batch_size=32, epochs=1, patience=10):\n",
        "            #\n",
        "            print(\"{}, training started...\".format(datetime.now()))\n",
        "            best_value = np.Inf\n",
        "            best_weights = None\n",
        "            wait = 0\n",
        "            # 自定义训练过程\n",
        "            data_generator = CrfGenerator(train_file, batch_size, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "            for epoch in range(epochs):\n",
        "                steps = len(data_generator)\n",
        "                loss_sum = 0\n",
        "                acc_sum = 0\n",
        "                for step in range(steps):\n",
        "                    x, y = data_generator.__getitem__(step)\n",
        "                    inputs, lengths, tags = x\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        logits = self.model(x, training=True)\n",
        "                        loss = self.crf_layer.compute_loss(logits, lengths, tags)\n",
        "                        acc = self.crf_layer.compute_accuracy(logits, lengths, tags)\n",
        "                        loss_sum += loss\n",
        "                        acc_sum += acc\n",
        "                    train_vars = [v for layer in [self.blstm, self.dense1, self.crf_layer]\n",
        "                                  for v in layer.trainable_variables]\n",
        "                    grads = tape.gradient(loss, train_vars)\n",
        "                    # print(type(self.model.trainable_variables))\n",
        "                    # Run one step of gradient descent by updating\n",
        "                    # the value of the variables to minimize the loss.\n",
        "                    self.model.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "                    print(\"{}, epoch: {}/{}, step: {}/{}, loss: {:.3f}, acc: {:.3f}\".format(\n",
        "                        datetime.now(), epoch+1, epochs, step+1, steps, loss, acc))\n",
        "                #\n",
        "                loss_avg = loss_sum / steps\n",
        "                acc_avg = acc_sum / steps\n",
        "                if np.less(loss_avg, best_value):\n",
        "                    best_value = loss_avg\n",
        "                    best_weights = self.model.get_weights()\n",
        "                    self.save_model()\n",
        "                    print(\"{}, found better model at epoch-{}. avg loss: {:.3f}, avg acc: {:.3f}\".format(\n",
        "                        datetime.now(), epoch, loss_avg, acc_avg\n",
        "                    ))\n",
        "                    #\n",
        "                    print(\"{}, validation started...\".format(datetime.now()))\n",
        "                    valid_generator = CrfGenerator(valid_file, None, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "                    for step in range(len(valid_generator)):\n",
        "                        x, y = valid_generator.__getitem__(step)\n",
        "                        x = [xx[0:10] for xx in x]\n",
        "                        y = y[0:10]\n",
        "                        inputs, lengths, tags = x\n",
        "                        logits = self.model(x, training=False)\n",
        "                        loss = self.crf_layer.compute_loss(logits, lengths, tags)\n",
        "                        acc = self.crf_layer.compute_accuracy(logits, lengths, tags)\n",
        "                        print(\"{}, validation, loss: {:.3f}, acc: {:.3f}\".format(\n",
        "                            datetime.now(), loss, acc\n",
        "                        ))\n",
        "                    print(\"{}, validation done.\".format(datetime.now()))\n",
        "                    print(\"{}, test started...\".format(datetime.now()))\n",
        "                    test_generator = CrfGenerator(test_file, None, self.max_seq_len, self.vocab2id, self.tag2id)\n",
        "                    for step in range(len(test_generator)):\n",
        "                        x, y = test_generator.__getitem__(step)\n",
        "                        x = [xx[0:10] for xx in x]\n",
        "                        y = y[0:10]\n",
        "                        inputs, lengths, tags = x\n",
        "                        logits = self.model(x, training=False)\n",
        "                        loss = self.crf_layer.compute_loss(logits, lengths, tags)\n",
        "                        acc = self.crf_layer.compute_accuracy(logits, lengths, tags)\n",
        "                        print(\"{}, test, loss: {:.3f}, acc: {:.3f}\".format(\n",
        "                            datetime.now(), loss, acc\n",
        "                        ))\n",
        "                    print(\"{}, test done.\".format(datetime.now()))\n",
        "                else:\n",
        "                    wait += 1\n",
        "                    if wait >= patience:\n",
        "                        self.model.set_weights(best_weights)\n",
        "                        self.save_model()\n",
        "                        print(\"{}, early stopped at epoch-{}\".format(\n",
        "                            datetime.now(), epoch\n",
        "                        ))\n",
        "                # 每个epoch结束后都随机改变下训练数据的顺序\n",
        "                data_generator.shuffle()\n",
        "            print(\"{}, training done.\".format(datetime.now()))\n",
        "\n",
        "        def save_model(self, model_path=None):\n",
        "            if model_path is None:\n",
        "                model_path = self.model_path\n",
        "            # with self.graph.as_default():\n",
        "            if self.is_save_by_weights:\n",
        "                self.model.save_weights(model_path)\n",
        "            else:\n",
        "                self.model.save(model_path)\n",
        "\n",
        "        def load_model(self, model_path=None):\n",
        "            if model_path is None:\n",
        "                model_path = self.model_path\n",
        "            # with self.graph.as_default():\n",
        "            if self.is_save_by_weights:\n",
        "                self.model.load_weights(model_path)\n",
        "            else:\n",
        "                self.model = tf.keras.models.load_model(\n",
        "                    model_path, custom_objects={\"CrfLayer\": CrfLayer})\n",
        "\n",
        "        def predict(self, text):\n",
        "            text_length = len(text)\n",
        "            words = np.array([[self.vocab2id.get(text[i] if i < len(text) else \"unknown\", 0)\n",
        "                               for i in range(self.max_seq_len)]])\n",
        "            tags = np.array([[self.tag2id.get('O') for _ in range(self.max_seq_len)]])\n",
        "            lengths = np.array([len(words[0])])\n",
        "            # with self.graph.as_default():\n",
        "            outputs = self.model([words, lengths, tags])\n",
        "            pred_ids = self.crf_layer.decode(outputs, lengths)\n",
        "            # print(pred_ids)\n",
        "            pred_ids = pred_ids.numpy().tolist()[0]\n",
        "            pred_tags = [self.id2tag.get(_id, \"O\") for _id in pred_ids[0: text_length]]\n",
        "            return pred_tags\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # import torch\n",
        "    #\n",
        "    # class BlstmCrfOnTorch(torch.nn):\n",
        "    #     def __init__(self):\n",
        "    #         super(BlstmCrfOnTorch, self).__init__()\n",
        "\n",
        "    #\n",
        "    f = \"tmp/train.txt\"\n",
        "    vocab2id, tag2id = get_data(f)\n",
        "    params = {\"vocab2id\": vocab2id, \"hidden_size\": 200, \"embed_size\": 1000,\n",
        "              \"tag2id\": tag2id, \"batch_size\": 32, \"max_seq_len\": 64,\n",
        "              \"is_save_by_weights\": True, \"model_path\": \"model.h5\",\n",
        "              \"pre_trained_model_path\": \"tmp\\\\chinese_L-12_H-768_A-12\"}\n",
        "    batch_size = params.get(\"batch_size\")\n",
        "\n",
        "    #\n",
        "    class ToyModel:\n",
        "        def __init__(self, kwargs):\n",
        "            self.layer_name_prefix = \"ToyModel_\"\n",
        "            #\n",
        "            kwargs = kwargs if kwargs is not None else dict()\n",
        "            self.is_save_by_weights = kwargs.get(\"is_save_by_weights\", True)\n",
        "            self.model_path = kwargs.get(\"model_path\", \"model.h5\")\n",
        "            max_seq_length = kwargs.get(\"max_seq_len\", 128)\n",
        "            self.max_seq_len = max_seq_length\n",
        "            vocab2id = kwargs.get(\"vocab2id\", None)\n",
        "            if vocab2id is None:\n",
        "                raise ValueError(\"vocab2id缺失!\")\n",
        "            vocab_size = len(vocab2id)\n",
        "            embed_size = kwargs.get(\"embed_size\", 100)\n",
        "            hidden_size = kwargs.get(\"hidden_size\", 100)\n",
        "            tag2id = kwargs.get(\"tag2id\", None)\n",
        "            if tag2id is None:\n",
        "                raise ValueError(\"tg2id缺失!\")\n",
        "            out_size = len(tag2id)\n",
        "            self.vocab_size = vocab_size\n",
        "            self.out_size = out_size\n",
        "            self.vocab2id = vocab2id\n",
        "            self.tag2id = tag2id\n",
        "            self.id2tag = dict([(i, t) for t, i in self.tag2id.items()])\n",
        "            #\n",
        "            embedding = tf.keras.layers.Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embed_size,\n",
        "                input_shape=(None, max_seq_length),\n",
        "                name=self.layer_name_prefix + \"embedding\",\n",
        "                dtype=tf.float32,\n",
        "                trainable=True)\n",
        "\n",
        "            dense1 = tf.keras.layers.Dense(out_size, activation=\"relu\", dtype=tf.float32,\n",
        "                                           name=self.layer_name_prefix + \"dense1\")\n",
        "            softmax = tf.keras.layers.Softmax(name=self.layer_name_prefix + \"softmax\")\n",
        "            inputs = tf.keras.Input(shape=(self.max_seq_len,))\n",
        "            outputs = embedding(inputs)\n",
        "            outputs = dense1(outputs)\n",
        "            outputs = tf.reduce_sum(outputs, axis=1)\n",
        "            outputs = softmax(outputs)\n",
        "            model2 = tf.keras.Model(inputs, outputs)\n",
        "            model2.compile(optimizer=\"Adam\", loss=tf.keras.losses.sparse_categorical_crossentropy)\n",
        "            model2.summary()\n",
        "            self.model = model2\n",
        "\n",
        "        def fit(self):\n",
        "            batch_size = 32\n",
        "            self.model.fit(\n",
        "                x=np.random.randint(0, self.vocab_size, (4096, self.max_seq_len)),\n",
        "                y=np.random.randint(0, self.out_size, (4096,)),\n",
        "                epochs=3,\n",
        "                batch_size=batch_size,\n",
        "                steps_per_epoch=math.ceil(4096/batch_size))\n",
        "            self.model.save(\"model_b.h5\")\n",
        "\n",
        "        def load_model(self, model_path):\n",
        "            self.model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "        def predict(self, text):\n",
        "            text_length = len(text)\n",
        "            words = np.array([[self.vocab2id.get(text[i] if i < len(text) else \"unknown\", 0)\n",
        "                               for i in range(self.max_seq_len)]])\n",
        "            # with self.graph.as_default():\n",
        "            outputs = self.model(words)\n",
        "            pred_ids = np.argmax(outputs, axis=-1).tolist()\n",
        "            return pred_ids\n",
        "\n",
        "    class TestTwoKerasModelAtTheSameTime:\n",
        "        def __init__(self):\n",
        "            self.model_a = Ner(kwargs)\n",
        "            self.model_a.load_model(model_path=\"model_a.h5\")\n",
        "            self.model_b = ToyModel(kwargs)\n",
        "            self.model_b.load_model(\"model_b.h5\")\n",
        "\n",
        "        def process(self, text):\n",
        "            result_a = self.model_a.predict(text)\n",
        "            result_b = self.model_b.predict(text)\n",
        "            print(\"result by model a: {}\".format(result_a))\n",
        "            print(\"result by model b: {}\".format(result_b))\n",
        "\n",
        "    dev_f = \"tmp/dev.txt\"\n",
        "    test_f = \"tmp/test.txt\"\n",
        "    # model = ToyModel(kwargs)\n",
        "    # model.fit()\n",
        "    model = Ner(params)\n",
        "    # model.fit(f, dev_f, test_f, batch_size=batch_size, epochs=1, steps_per_epoch=3)\n",
        "    model.train(f, dev_f, test_f, batch_size=batch_size, epochs=5)\n",
        "    model.load_model(\"model.h5\")\n",
        "    tags = model.predict(\"雅诗兰黛防晒霜好用吗\")\n",
        "    print(tags)\n",
        "    #\n",
        "    # handler = TestTwoKerasModelAtTheSameTime()\n",
        "    # while True:\n",
        "    #     t = input(\"请输入: \")\n",
        "    #     handler.process(t)\n",
        "print(\"加载完毕.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "加载完毕.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GuC1SkuOwl_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}